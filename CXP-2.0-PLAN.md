# CXP - Universal AI Context Format

---

## ğŸ“Š AKTUELLER FORTSCHRITT (Stand: 26. Dezember 2025)

```
Phase 1: Core Library      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% âœ…
Phase 2: Embeddings/Search â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  80% ğŸ”„
Phase 3: Extension System  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% â³
Phase 4: Multi-Platform    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% â³
Phase 5: ContextAI         â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% â³

Gesamt:                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  36%
```

### âœ… Was funktioniert JETZT:
- `cxp build /path output.cxp` - CXP-Dateien erstellen
- `cxp info file.cxp` - Statistiken anzeigen
- `cxp list file.cxp` - Dateien auflisten
- `cxp extract file.cxp` - Dateien extrahieren
- FastCDC Chunking mit Deduplication
- Zstandard Kompression (85% kleiner als JSON!)
- Binary Embeddings (32x kleiner als float32)
- Int8 Embeddings fÃ¼r Rescoring
- HNSW Index mit Hamming Distance
- **15/15 Integration Tests bestanden**

### â³ Was fehlt noch:
- Query Engine (`cxp query file.cxp "search term"`)
- Embeddings in CXP-Datei speichern
- Extension System fÃ¼r ContextAI
- WASM Build fÃ¼r Browser
- Node.js/Python Bindings
- SQLite Migration Tool

### ğŸ“ Implementierte Module (cxp-core/src/):
```
lib.rs              âœ… Public API, Feature Flags
format.rs           âœ… CxpBuilder, CxpReader (ZIP Container)
chunker.rs          âœ… FastCDC Content-Defined Chunking
dedup.rs            âœ… SHA256 Deduplication
compress.rs         âœ… Zstandard Compression
manifest.rs         âœ… Manifest mit Stats & FileTypes
error.rs            âœ… CxpError Enum (12 Varianten)
embeddings.rs       âœ… ONNX Runtime Engine (ort 2.0.0-rc.10)
embeddings_tract.rs âœ… Tract Engine fÃ¼r WASM (tract-onnx 0.22)
index.rs            âœ… HNSW Index (usearch 2.15)
```

---

## Vision
Ein **offenes, universelles Datenformat** fÃ¼r KI-Anwendungen:
- **$0 Kosten** - Komplett lokal, keine API-Calls
- **Ãœberall lauffÃ¤hig** - Rust, WASM, Node.js, Python
- **Ersetzt SQLite** - Eine Datei statt Datenbank
- **Multi-KI Ready** - Claude, GPT, Gemini, Llama...
- **Zukunftssicher** - Erweiterbar fÃ¼r neue Use Cases
- **Open Standard** - Jeder kann es nutzen/implementieren

## Die groÃŸe Idee
CXP wird das **"PDF fÃ¼r KI"** - ein universelles Format das:
1. Jede KI lesen kann
2. Semantische Suche built-in hat
3. Komplette App-States speichern kann
4. Portabel und offline funktioniert
5. Ein offener Standard werden kann

---

## Use Cases (Heute & Zukunft)

### 1. ContextAI (Erste Implementation)
```
ContextAI App - SQLite wird komplett ersetzt durch CXP!

Vorher (SQLite):           Nachher (CXP):
â”œâ”€â”€ 7 Tabellen             â”œâ”€â”€ 1 Datei: context.cxp
â”œâ”€â”€ Keyword-Suche          â”œâ”€â”€ Semantische Suche
â”œâ”€â”€ App-gebunden           â”œâ”€â”€ Portabel
â””â”€â”€ Nicht teilbar          â””â”€â”€ Einfach kopieren/teilen
```

### 2. Personal Knowledge Base
```
Alle deine Dokumente, Notizen, Code in einer .cxp Datei
â†’ Frag jede KI Fragen Ã¼ber DEINE Daten
â†’ Lokal, privat, keine Cloud
```

### 3. Projekt-Kontext fÃ¼r Entwickler
```
my-project.cxp
â†’ EnthÃ¤lt komplette Codebase mit Embeddings
â†’ Cursor, Windsurf, Claude Code kÃ¶nnen es laden
â†’ "Versteh mein Projekt" in einer Datei
```

### 4. Team Knowledge Sharing
```
team-knowledge.cxp
â†’ Team teilt Wissen in einer Datei
â†’ Neue Mitarbeiter: Datei laden â†’ KI kennt alles
â†’ Kein Onboarding-Chaos mehr
```

### 5. Zukunft: Universal AI Data Layer
```
Alle Apps speichern in .cxp
â†’ Deine Daten gehÃ¶ren DIR
â†’ Wechsel zwischen KIs/Apps ohne Datenverlust
â†’ InteroperabilitÃ¤t zwischen Tools
```

---

## Multi-Platform Architektur

CXP lÃ¤uft **Ã¼berall**:

```
cxp/
â”œâ”€â”€ cxp-core/              # Rust Core Library
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs         # Public API
â”‚   â”‚   â”œâ”€â”€ format.rs      # CXP Read/Write
â”‚   â”‚   â”œâ”€â”€ chunker.rs     # FastCDC
â”‚   â”‚   â”œâ”€â”€ embeddings.rs  # ONNX Runtime
â”‚   â”‚   â”œâ”€â”€ index.rs       # HNSW Search
â”‚   â”‚   â”œâ”€â”€ quantize.rs    # Binary/Int8
â”‚   â”‚   â””â”€â”€ extensions.rs  # Namespace System
â”‚   â””â”€â”€ Cargo.toml
â”‚
â”œâ”€â”€ cxp-wasm/              # WebAssembly Build
â”‚   â””â”€â”€ (Browser, Deno, Cloudflare Workers)
â”‚
â”œâ”€â”€ cxp-node/              # Node.js Bindings (napi-rs)
â”‚   â””â”€â”€ (npm package: @cxp/core)
â”‚
â”œâ”€â”€ cxp-python/            # Python Bindings (PyO3)
â”‚   â””â”€â”€ (pip package: cxp)
â”‚
â”œâ”€â”€ cxp-cli/               # Standalone CLI
â”‚   â””â”€â”€ cxp build, cxp query, cxp export
â”‚
â””â”€â”€ schemas/               # FlatBuffers Schemas
    â”œâ”€â”€ manifest.fbs
    â”œâ”€â”€ embeddings.fbs
    â””â”€â”€ extensions/
        â””â”€â”€ contextai.fbs
```

### Platform Support Matrix

| Platform | Runtime | Use Case |
|----------|---------|----------|
| **Tauri/Desktop** | Rust Native | ContextAI App |
| **Browser** | WASM | Web Apps, PWAs |
| **Node.js** | napi-rs | CLI Tools, Servers |
| **Python** | PyO3 | Data Science, ML |
| **Deno** | WASM | Edge Functions |
| **Mobile** | Rust FFI | iOS/Android Apps |

---

## CXP als SQLite-Ersatz fÃ¼r ContextAI

### Aktuelles SQLite Schema (wird ersetzt):
```sql
files, conversations, chat_messages, context_log,
user_habits, habit_history, watched_folders,
browser_history, custom_dictionary
```

### Neues CXP Format:
```
context.cxp (ZIP Container)
â”œâ”€â”€ core/                      # Standard CXP
â”‚   â”œâ”€â”€ manifest.fbs           # Metadata, Version, Stats
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ binary.bin         # Binary Embeddings (48B/vec)
â”‚   â”‚   â”œâ”€â”€ int8.bin           # Int8 fÃ¼r Rescoring
â”‚   â”‚   â””â”€â”€ index.hnsw         # HNSW Index
â”‚   â”œâ”€â”€ chunks/
â”‚   â”‚   â””â”€â”€ *.zst              # Zstandard komprimiert
â”‚   â””â”€â”€ file_map.msgpack       # Datei â†’ Chunks
â”‚
â””â”€â”€ contextai/                 # ContextAI Extension
    â”œâ”€â”€ conversations/
    â”‚   â”œâ”€â”€ index.msgpack      # Conversation List
    â”‚   â””â”€â”€ conv_*.msgpack     # Individual Conversations
    â”œâ”€â”€ habits.msgpack         # User Preferences
    â”œâ”€â”€ dictionary.msgpack     # Custom Terms
    â”œâ”€â”€ watched_folders.msgpack
    â””â”€â”€ settings.msgpack       # App Settings
```

### Migration Path:
```
1. CXP Library implementieren
2. ContextAI: SQLite â†’ CXP Adapter
3. Migration Tool: SQLite â†’ CXP Export
4. SQLite Code entfernen
5. Nur noch CXP
```

---

## Neue Erkenntnisse aus der Recherche

### 1. Embedding-Modelle (2025 State-of-the-Art)

| Modell | Size | Dims | Besonderheit |
|--------|------|------|--------------|
| **EmbeddingGemma** | 308M / ~200MB RAM | 768 (MRL: 512/256/128) | Best-in-class fÃ¼r On-Device, int4 quantized |
| **all-MiniLM-L6-v2** | 22M / ~90MB | 384 | BewÃ¤hrt, schnell |
| **BGE-small** | 33M | 384 | Multilingual |

**Breakthrough:** EmbeddingGemma mit Matryoshka (MRL) erlaubt flexible Dimensionen + int4 Quantisierung = **32x kleinere Vektoren**!

### 2. Binary Embeddings (Game-Changer!)

```
float32 (384 dims) = 1.5 KB pro Vector
int8 (384 dims)    = 384 Bytes (4x kleiner)
binary (384 dims)  = 48 Bytes (32x kleiner!)
```

**Strategie:**
1. Binary Search (48 Bytes) fÃ¼r Vorfilterung
2. int8 Rescoring fÃ¼r Top-100
3. Optional: Reranking fÃ¼r Top-10

**Ergebnis:** 95% QualitÃ¤t bei 32x weniger Speicher!

### 3. WebGPU Embeddings im Browser

- **Transformers.js v3** mit WebGPU: **64x schneller** als WASM!
- LÃ¤uft direkt im Browser, keine Server-Kosten
- 70% Browser-Support (Chrome, Edge, Firefox)
- Fallback auf WASM fÃ¼r Ã¤ltere Browser

### 4. HNSW im Browser (WASM)

- **EdgeVec** (Rust/WASM): 148KB Bundle, sub-ms Search bei 100k Vectors
- **hnswlib-wasm**: Browser HNSW mit IndexedDB Persistenz
- **USearch**: Cross-platform, SIMD-optimiert

### 5. Bessere Serialisierung

| Format | vs JSON | Zero-Copy | Use-Case |
|--------|---------|-----------|----------|
| **FlatBuffers** | 80% kleiner | JA | Manifest, schneller Zugriff |
| **MessagePack** | 70% kleiner | Nein | Flexible Daten |
| **Protobuf** | 80% kleiner | Nein | Embeddings |

**Neu:** FlatBuffers fÃ¼r Manifest = Zero-Copy Zugriff ohne Parsing!

### 6. Semantic Chunking (2025)

- **FastCDC** mit Gear Hash: O(log N) Chunking
- **HOPE Metric:** Semantische UnabhÃ¤ngigkeit optimieren
- **Hashless CDC:** Noch schneller, keine Rolling Hashes

### 7. LMCompress (Neural Compression)

- Halbiert JPEG-XL, FLAC, H.264
- Text: 1/3 der zpaq-GrÃ¶ÃŸe
- **Aber:** Zu compute-intensiv fÃ¼r $0-Ziel

---

## CXP 2.0 Architektur (Optimiert)

```
contextpack.cxp (ZIP Container)
â”œâ”€â”€ manifest.fbs          # FlatBuffers (Zero-Copy, 5-15KB)
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ binary.bin        # Binary Embeddings (48B/vector) - Primary
â”‚   â”œâ”€â”€ int8.bin          # Int8 fÃ¼r Rescoring (384B/vector) - Optional
â”‚   â””â”€â”€ index.hnsw        # HNSW Index (WASM-kompatibel)
â”œâ”€â”€ chunks/
â”‚   â””â”€â”€ *.zst             # Zstandard komprimiert
â”œâ”€â”€ file_map.msgpack      # Datei â†’ Chunks
â”œâ”€â”€ keywords.fst          # FST statt Trie (kleiner, schneller)
â””â”€â”€ meta.cbor             # ZusÃ¤tzliche Metadaten
```

### GrÃ¶ÃŸenvergleich

```
Original CXP (Spec):     170MB fÃ¼r 500MB Input
CXP 2.0 (Binary Emb):    ~50MB fÃ¼r 500MB Input (70% kleiner!)
```

---

## Zero-Cost Pipeline

### Phase 1: Chunking ($0, lokal)
```
Input Files â†’ FastCDC (Gear Hash) â†’ SHA256 Dedup â†’ Chunks
```
- Rust native
- 100% lokal, keine Dependencies auÃŸer Crypto

### Phase 2: Embeddings ($0, lokal)
```
Chunks â†’ ONNX Runtime â†’ Binary Quantization
```
- **Desktop:** ONNX Runtime (native)
- **Browser:** WebGPU oder WASM
- **Model:** EmbeddingGemma (200MB, On-Demand)

### Phase 3: Index ($0, lokal)
```
Binary Embeddings â†’ HNSW Build â†’ .hnsw File
```
- usearch (Rust native)
- WASM-kompatibel fÃ¼r Browser

### Phase 4: Search ($0, lokal)
```
Query â†’ Embed â†’ Binary HNSW Search â†’ Int8 Rescore â†’ Top-K
```
- Alles lokal!
- Keine Server, keine API-Kosten

---

## Neue Features (Ideen zum Nachdenken)

### 1. Progressive Loading
```
Lade zuerst: manifest.fbs (5KB) + binary.bin Header
Dann on-demand: Chunks nur wenn gebraucht
```
â†’ Instant Start, selbst fÃ¼r GB-groÃŸe CXP-Dateien

### 2. Streaming Embeddings
```
WÃ¤hrend User tippt â†’ Query Embedding berechnen
Binary Search startet sofort â†’ Latenz maskiert
```

### 3. Differential Updates (Delta-CXP)
```
Original: data.cxp (50MB)
Update:   delta.cxp (500KB) - nur geÃ¤nderte Chunks
Merge:    Lazy, on-demand
```

### 4. Peer-to-Peer Sharing
```
CXP ist eine Datei â†’ Torrent, IPFS, lokales Netzwerk
Kein Server nÃ¶tig fÃ¼r Sharing
```

### 5. Multi-Modal ohne API
```
Images: CLIP ONNX (lokal)
Audio:  Whisper ONNX (lokal) â†’ Text â†’ Embedding
PDF:    pdf.js â†’ Text â†’ Embedding
```
Alles $0!

### 6. Hybrid Intelligence
```
Lokal:    Binary Search + Int8 Rescore (95% accuracy)
Optional: Claude fÃ¼r Top-3 Ergebnisse ($0.01/query)
```
â†’ 95% der Queries komplett kostenlos

---

## Umwelt-Impact

| Ansatz | CO2/Query (geschÃ¤tzt) |
|--------|----------------------|
| GPT-4 API | ~4.5g CO2 |
| Claude API | ~2-3g CO2 |
| **CXP (lokal)** | ~0.01g CO2 |

**Faktor 200-400x weniger CO2** durch lokale Verarbeitung!

---

## Technische Entscheidungen

### Embedding Model
**GewÃ¤hlt:** `EmbeddingGemma` (Google, 2025)
- 308M Parameter, ~200MB
- 768 dims (MRL: flexible 512/256/128)
- int4 quantized out-of-the-box
- Best-in-class fÃ¼r On-Device
- Multilingual (100+ Sprachen)
- On-Demand Download (nicht bundled)

### Quantization
**GewÃ¤hlt:** Binary + Int8 Hybrid
- Binary fÃ¼r HNSW Index (schnell, klein)
- Int8 fÃ¼r Rescoring (bessere PrÃ¤zision)
- ~3% QualitÃ¤tsverlust, 32x Speicherersparnis

### Serialisierung
**GewÃ¤hlt:**
- Manifest: FlatBuffers (Zero-Copy)
- File Map: MessagePack (flexibel)
- Embeddings: Raw Binary (effizient)

### Search
**GewÃ¤hlt:** HNSW via usearch
- Sub-ms bei 100k Vectors
- Rust native + WASM Support

---

## Implementation Roadmap

### Phase 1: CXP Core Library âœ… KOMPLETT
- [x] Rust Workspace Setup (cxp-core, cxp-cli)
- [x] MessagePack fÃ¼r Manifest (statt FlatBuffers - einfacher)
- [x] FastCDC Chunking (Gear Hash)
- [x] SHA256 Deduplication
- [x] Zstandard Compression
- [x] ZIP Container Read/Write
- [x] CLI: `cxp build`, `cxp info`, `cxp list`, `cxp extract`

### Phase 2: Embeddings & Search ğŸ”„ 80% FERTIG
- [x] ONNX Runtime Integration (`ort = "2.0.0-rc.10"`)
- [x] WASM-Alternative mit tract-onnx (`tract-onnx = "0.22"`)
- [x] Model Support (all-MiniLM-L6-v2, EmbeddingGemma)
- [x] Binary Quantization (float32 â†’ binary, 32x kleiner!)
- [x] Int8 Quantization fÃ¼r Rescoring (4x kleiner)
- [x] HNSW Index Build (`usearch = "2.15"`)
- [x] Hamming Distance fÃ¼r Binary Embeddings
- [x] **15/15 Integration Tests bestanden**
- [ ] Query Engine: `cxp query file.cxp "search term"`
- [ ] Embeddings in CXP-Datei integrieren

### Phase 3: Extension System â³
- [ ] Namespace System fÃ¼r Extensions
- [ ] ContextAI Extension Schema
- [ ] Conversations Storage
- [ ] Habits/Dictionary/Settings Storage
- [ ] CLI: `cxp ext add contextai`

### Phase 4: Multi-Platform â³
- [ ] WASM Build (wasm-pack) - tract-onnx Grundlage vorhanden!
- [ ] Node.js Bindings (napi-rs)
- [ ] Python Bindings (PyO3)
- [ ] npm/pip Package Publishing

### Phase 5: ContextAI Integration â³
- [ ] SQLite â†’ CXP Migration Tool
- [ ] Tauri Commands fÃ¼r CXP
- [ ] Frontend anpassen
- [ ] SQLite Code entfernen
- [ ] Testing & Bug Fixes

---

### Aktuelle Crate Versionen (Getestet & Funktionierend)

```toml
[workspace]
members = ["cxp-core", "cxp-cli"]

[workspace.dependencies]
# Core
fastcdc = "3.1"
zstd = "0.13"
sha2 = "0.10"
zip = "2.2"
rayon = "1.10"

# Serialization
flatbuffers = "24.12"
rmp-serde = "1.3"
serde = { version = "1", features = ["derive"] }
serde_json = "1.0"

# Error Handling
thiserror = "2.0"
anyhow = "1.0"

# Logging
tracing = "0.1"

# File System
walkdir = "2.5"

# Misc
chrono = "0.4"
uuid = { version = "1.11", features = ["v4"] }
hex = "0.4"

# Embeddings (optional) - NATIVE (schnell)
ort = { version = "2.0.0-rc.10", optional = true }
ndarray = { version = "0.16", optional = true }
tokenizers = { version = "0.21", optional = true }
num_cpus = { version = "1.16", optional = true }

# Embeddings (optional) - WASM KOMPATIBEL (portabel)
tract-onnx = { version = "0.22", optional = true }

# Search (optional)
usearch = { version = "2.15", optional = true }

[features]
default = []
embeddings = ["ort", "ndarray", "tokenizers", "num_cpus"]
embeddings-wasm = ["tract-onnx", "ndarray", "tokenizers"]
search = ["usearch"]
```

---

## Entschieden

1. **Vision:** CXP als universelles KI-Datenformat (Potenzial fÃ¼r Standard)
2. **Model:** EmbeddingGemma (200MB, On-Demand Download)
3. **Plattform:** Multi-Platform (Rust + WASM + Node + Python)
4. **Manifest:** FlatBuffers (Zero-Copy)
5. **ContextAI:** SQLite wird komplett durch CXP ersetzt
6. **Extension System:** Namespaces fÃ¼r App-spezifische Daten

---

## Lizenz & Schutz Strategie

### Phase 1: Build in Private (JETZT)
```
â”œâ”€â”€ Private GitHub Repository
â”œâ”€â”€ Kein Ã¶ffentlicher Code
â”œâ”€â”€ Fokus auf Bauen, nicht Marketing
â””â”€â”€ Niemand sieht was du machst
```

### Phase 2: Teaser & Hype (Wenn MVP fertig)
```
â”œâ”€â”€ Twitter/X: "Building something new..."
â”œâ”€â”€ Screenshots & Demo Videos
â”œâ”€â”€ Waitlist aufbauen
â”œâ”€â”€ KEIN Code zeigen
â””â”€â”€ Interesse wecken
```

### Phase 3: Launch (App Release)
```
â”œâ”€â”€ ContextAI App = Closed Source
â”œâ”€â”€ CXP Format Spec = Noch nicht verÃ¶ffentlichen
â”œâ”€â”€ User kÃ¶nnen App nutzen
â””â”€â”€ Format bleibt "Black Box" fÃ¼rs Erste
```

### Phase 4: Open Standard (Optional, spÃ¤ter)
```
Wenn du bereit bist:
â”œâ”€â”€ CXP Spec unter AGPL-3.0 verÃ¶ffentlichen
â”œâ”€â”€ Commercial License fÃ¼r Firmen anbieten
â”œâ”€â”€ Community aufbauen
â””â”€â”€ Standard-Adoption anstreben

ODER proprietÃ¤r bleiben - du entscheidest spÃ¤ter
```

### Warum dieser Ansatz gut ist:
```
âœ“ Maximaler Schutz wÃ¤hrend du baust
âœ“ Kein Stress wegen Konkurrenz
âœ“ FlexibilitÃ¤t fÃ¼r spÃ¤ter
âœ“ Hype aufbauen ohne Code zu zeigen
âœ“ Du behÃ¤ltst alle Optionen
```

## ContextAI App Pfad
`/Users/einarjaeger/Documents/GitHub/context Ai App`

## Projekt-Struktur (dieser Ordner)
```
/Users/einarjaeger/Documents/GitHub/cpx.datei typ/
â”œâ”€â”€ cxp-core/          # Rust Core Library
â”œâ”€â”€ cxp-cli/           # CLI Tool
â”œâ”€â”€ cxp-wasm/          # WASM Build
â”œâ”€â”€ cxp-node/          # Node.js Bindings
â”œâ”€â”€ cxp-python/        # Python Bindings
â”œâ”€â”€ schemas/           # FlatBuffers Schemas
â”œâ”€â”€ docs/              # Spezifikation & Docs
â”‚   â””â”€â”€ SPEC.md        # Offizielle CXP Spezifikation
â”œâ”€â”€ examples/          # Beispiele
â”œâ”€â”€ CXP-2.0-PLAN.md    # Dieser Plan
â””â”€â”€ cpx.newdatatyp.md  # Original Spec
```

---

## Quellen

### Embedding Models
- [EmbeddingGemma - Google](https://developers.googleblog.com/en/introducing-embeddinggemma/)
- [Transformers.js v3 WebGPU](https://huggingface.co/blog/transformersjs-v3)
- [FastEmbed - Qdrant](https://github.com/qdrant/fastembed)

### Quantization
- [Binary & Scalar Embedding Quantization - HuggingFace](https://huggingface.co/blog/embedding-quantization)
- [Matryoshka Embeddings - Vespa](https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/)
- [Voyage AI Quantization](https://blog.voyageai.com/2025/05/20/voyage-3-5/)

### Chunking
- [FastCDC Paper - USENIX](https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia)
- [Semantic Chunking 2025](https://www.emergentmind.com/topics/content-defined-chunking-cdc)

### Search
- [LSH Guide - Pinecone](https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/)
- [USearch - HNSW](https://github.com/unum-cloud/USearch)
- [EdgeVec - Browser Vector Search](https://news.ycombinator.com/item?id=46249896)
- [hnswlib-wasm](https://github.com/ShravanSunder/hnswlib-wasm)

### Serialization
- [FlatBuffers Benchmarks](https://flatbuffers.dev/benchmarks/)
- [Binary Format Comparison](https://www.cloudthat.com/resources/blog/optimizing-api-performance-with-protocol-buffers-flatbuffers-messagepack-and-cbor)

### Compression
- [LMCompress - Nature](https://www.nature.com/articles/s42256-025-01033-7)
- [WebGPU Embedding Benchmark](https://huggingface.co/posts/Xenova/906785325455792)

---

*Erstellt: 2025-12-26*
*Status: Ready for Implementation*
